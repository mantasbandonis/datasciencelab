{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/mantasbandonis/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import csv\n",
    "import requests\n",
    "import json\n",
    "import operator\n",
    "import time\n",
    "import datetime\n",
    "import platform\n",
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeText(text, path, mode = 'a'):\n",
    "    with open (path, mode) as textout:\n",
    "        textout.write((text))\n",
    "        \n",
    "def writeJson(json, path, mode = 'w'):\n",
    "    with open(path, mode) as file:\n",
    "        file.write(json.dumps(json))\n",
    "        \n",
    "def writeCsv(listOut, outputFile):\n",
    "    import csv\n",
    "    with open (outputFile, \"w\", newline='') as outputfile:\n",
    "        writer = csv.writer(outputfile, delimiter = \",\")\n",
    "        for element in listOut:\n",
    "            writer.writerow(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTxt(path):\n",
    "    return open(path, 'r').read()\n",
    "\n",
    "def getCsv(path, delim = ','):\n",
    "    list_return = []\n",
    "    with open (path) as file:\n",
    "        csvreader = csv.reader(file, delimiter = delim)        \n",
    "        for i, line in enumerate(csvreader):\n",
    "            list_inner = []\n",
    "            list_inner.append(line[0].replace('\\\"','').replace('\\'','').replace('[','').replace(']','').strip().split(','))\n",
    "            list_inner.append(line[1].replace('\\\"','').replace('\\'','').replace('[','').replace(']','').strip().split(','))\n",
    "            list_inner.append(line[2].replace('\\\"','').replace('\\'','').replace('[','').replace(']','').strip().split(','))\n",
    "            list_return.append(list_inner)\n",
    "    \n",
    "    \n",
    "    return list_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if platform.system() == 'Windows':\n",
    "    prep_data = getCsv('..\\\\output\\\\prep_out.csv')\n",
    "else:\n",
    "    prep_data = getCsv('../output/prep_out.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['domestic', 'general', 'government', 'health', 'expenditure', 'caput', 'current', 'u', '$', 'country', 'public', 'expenditure', 'health', 'domestic', 'source', 'caput', 'expressed', 'current', 'u', 'dollar']\n"
     ]
    }
   ],
   "source": [
    "lemmatized_words2 =[]\n",
    "lemmatizer = nltk.WordNetLemmatizer()\n",
    "for sub_list in prep_data:\n",
    "    for sub_list2 in sub_list:\n",
    "        for value in sub_list2:\n",
    "            lemmatized_words = value.replace(\" \", \"\")\n",
    "            lemmatized_words = lemmatizer.lemmatize(lemmatized_words)\n",
    "            lemmatized_words2.append(lemmatized_words)\n",
    "print(lemmatized_words2[0:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "color\n",
      "caress\n",
      "pony\n",
      "presumably\n",
      "owed\n",
      "say\n"
     ]
    }
   ],
   "source": [
    "typea = ['colors', 'caresses', 'ponies', 'presumably', 'owed', 'says']\n",
    "for i in range(0,len(typea)):\n",
    "    # Lemmatize the words\n",
    "    lemmatized_words = lemmatizer.lemmatize(typea[i])\n",
    "    print(lemmatized_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "expenditure\n",
      "purchasing\n",
      "country\n",
      "transfer\n",
      "politics\n",
      "riding\n"
     ]
    }
   ],
   "source": [
    "print(lemmatizer.lemmatize('expenditures')) \n",
    "print(lemmatizer.lemmatize('purchasing')) \n",
    "print(lemmatizer.lemmatize('countries')) \n",
    "print(lemmatizer.lemmatize('transfers'))\n",
    "print(lemmatizer.lemmatize('politics')) \n",
    "print(lemmatizer.lemmatize('riding'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
