{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import csv\n",
    "import operator\n",
    "import time\n",
    "import datetime\n",
    "import platform\n",
    "import threading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeText(text, path, mode = 'w'):\n",
    "    with open (path, mode, encoding = 'utf-8') as textout:\n",
    "        textout.write((text))\n",
    "        \n",
    "def writeJson(json, path, mode = 'w'):\n",
    "    with open(path, mode) as file:\n",
    "        file.write(json.dumps(json))\n",
    "        \n",
    "def writeCsv(listOut, outputFile):\n",
    "    with open (outputFile, \"w\", newline='', encoding = 'utf-8') as outputfile:\n",
    "        writer = csv.writer(outputfile, delimiter = \",\")\n",
    "        for element in listOut:\n",
    "            writer.writerow(element)\n",
    "            \n",
    "def getTxt(path):\n",
    "    return open(path, 'r').read()\n",
    "\n",
    "def getCsv(path, delim = ','):\n",
    "    list_return = []\n",
    "    with open (path, encoding = 'utf-8') as file:\n",
    "        csvreader = csv.reader(file, delimiter = delim)        \n",
    "        for i, line in enumerate(csvreader):\n",
    "            list_return.append(line)\n",
    "    return list_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFloatCsv(path, delim = ','):\n",
    "    list_return = []\n",
    "    with open (path, encoding = 'utf-8') as file:\n",
    "        csvreader = csv.reader(file, delimiter = delim)        \n",
    "        for i, line in enumerate(csvreader):\n",
    "            list_return.append([float(x) for x in line])\n",
    "    return list_return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if platform.system() == 'Windows':\n",
    "    feat = getFloatCsv('..\\\\output\\\\feat.csv')\n",
    "else:\n",
    "    feat = getFloatCsv('../output/feat.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if platform.system() == 'Windows':\n",
    "    label = getCsv('..\\\\output\\\\labels.csv')\n",
    "else:\n",
    "    label = getCsv('../output/labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### numpy prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(feat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sklearn prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "multilabel_binarizer = MultiLabelBinarizer()\n",
    "multilabel_binarizer.fit(label)\n",
    "y = multilabel_binarizer.transform(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.40620422,  0.37869263,  0.58084106, ...,  0.03137207,\n",
       "          0.64916992, -1.32333374],\n",
       "        [-0.50666809,  0.45892334,  0.73953247, ..., -0.08215332,\n",
       "          1.0402832 , -1.47055054],\n",
       "        [-0.44416809,  0.43939209,  0.61599731, ..., -0.01086426,\n",
       "          0.69067383, -1.39877319],\n",
       "        ...,\n",
       "        [-0.03445435,  0.78988647, -0.72647095, ...,  0.727005  ,\n",
       "          1.64715576,  1.22302246],\n",
       "        [-0.02322388,  0.88021851, -0.77420044, ...,  0.44233704,\n",
       "          1.83660889,  1.10900879],\n",
       "        [-0.13491821,  0.87011719, -0.56777954, ...,  0.61347961,\n",
       "          2.03826904,  1.07580566]]), array([[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_X, test_X, train_y, test_y = train_test_split(X, y, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dims training set:  (2830, 300) (2830, 352)\n",
      "Dims training set:  (708, 300) (708, 352)\n"
     ]
    }
   ],
   "source": [
    "print('Dims training set: ', train_X.shape, train_y.shape)\n",
    "print('Dims training set: ', test_X.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gridResultRFC(data, n_est1, n_est2, n_est_grid, depth1, depth2, depth_grid, min_leaf1=0, min_leaf2=100, min_leaf_grid=50, filename = 'log_rf_grid', thread = 1):\n",
    "    train_X, test_X, train_y, test_y = data[0], data[1], data[2], data[3]\n",
    "    acc_best = 0\n",
    "    prec_best = 0\n",
    "    acc_params_best = [0,0,0]\n",
    "    prec_params_best = [0,0,0]\n",
    "    str_log = 'Started at:\\n' + datetime.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d %H:%M:%S') + '\\nThread: ' + str(thread) + '\\n'\n",
    "    print(str_log)\n",
    "    est_runs = (n_est2-n_est1)/n_est_grid\n",
    "    depth_runs = (depth2-depth1)/depth_grid\n",
    "    leaf_runs = (min_leaf2-min_leaf1)/min_leaf_grid\n",
    "    str_tmp = 'Grid Search will test ' + str(est_runs*depth_runs*leaf_runs) + ' combinations.\\n'\n",
    "    str_log += str_tmp\n",
    "    print(str_tmp)\n",
    "    \n",
    "    est_act = n_est1\n",
    "    depth_act = depth1\n",
    "    leaf_act = min_leaf1\n",
    "    \n",
    "    while (est_act < n_est2):\n",
    "        depth_act = depth1\n",
    "        while (depth_act < depth2):\n",
    "            leaf_act = min_leaf1\n",
    "            while (leaf_act < min_leaf2):\n",
    "                rf = RandomForestClassifier(n_estimators=est_act, max_depth=depth_act, min_samples_leaf=leaf_act)\n",
    "                rf.fit(train_X, train_y)\n",
    "                pred_rf = rf.predict(test_X)        \n",
    "                prec =  metrics.precision_score(test_y, pred_rf, average=\"samples\")\n",
    "                acc = metrics.accuracy_score(test_y, pred_rf)\n",
    "                print(datetime.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d %H:%M:%S'))\n",
    "                str_tmp = '----------------------\\n'+ 'Thread: ' + str(thread) + '\\n' + 'Params: '+str(est_act)+','+str(depth_act)+','+str(leaf_act)+'\\n'+'Accuracy: '+str(acc)+';'+' Precision: '+str(prec)+'\\n'\n",
    "                print('Accuracy: ', str(acc_best), '\\nParams: ', str(acc_params_best))\n",
    "                print('Precision: ', str(prec_best), '\\nParams: ', str(prec_params_best), '\\n')\n",
    "                str_log += str_tmp\n",
    "                print(str_tmp)\n",
    "                if acc_best <= acc:\n",
    "                    acc_best = acc\n",
    "                    acc_params_best = [est_act,depth_act,leaf_act]\n",
    "                if prec_best <= prec:\n",
    "                    prec_best = prec\n",
    "                    prec_params_best = [est_act,depth_act,leaf_act]\n",
    "                leaf_act += min_leaf_grid\n",
    "            depth_act += depth_grid\n",
    "        est_act += n_est_grid\n",
    "        \n",
    "    str_tmp = '==========================\\n==========================\\n\\n\\n'+'Accuracy: '+str(acc_best)+'\\nParams: '+str(acc_params_best) + '\\n' + 'Precision: '+str(prec_best)+'\\nParams: '+str(prec_params_best)+'\\n'+'Ended at:\\n'+datetime.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d %H:%M:%S')\n",
    "    str_log += str_tmp\n",
    "    \n",
    "    print('==========================\\n==========================\\n\\n')\n",
    "    print('Thread: ', thread)\n",
    "    print('Accuracy: ', str(acc_best), '\\nParams: ', str(acc_params_best))\n",
    "    print('Precision: ', str(prec_best), '\\nParams: ', str(prec_params_best), '\\n\\n')\n",
    "    print('Ended at:\\n' + datetime.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d %H:%M:%S'))\n",
    "    \n",
    "    writeText(str_log, ('../output/' + filename + '.txt'))             \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_train_set = [train_X, test_X, train_y, test_y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started at:\n",
      "2018-11-09 23:37:11\n",
      "Thread: 1\n",
      "\n",
      "Grid Search will test 18.651428571428568 combinations.\n",
      "\n",
      "Started at:\n",
      "2018-11-09 23:37:11\n",
      "Thread: 2\n",
      "Started at:\n",
      "2018-11-09 23:37:11\n",
      "Thread: 3\n",
      "\n",
      "\n",
      "Grid Search will test 18.651428571428568 combinations.\n",
      "\n",
      "Grid Search will test 18.651428571428568 combinations.\n",
      "\n",
      "Started at:\n",
      "2018-11-09 23:37:11\n",
      "Thread: 4\n",
      "\n",
      "Grid Search will test 18.651428571428568 combinations.\n",
      "\n",
      "Started at:\n",
      "2018-11-09 23:37:11\n",
      "Thread: 5\n",
      "\n",
      "Grid Search will test 18.651428571428568 combinations.\n",
      "\n",
      "Started at:\n",
      "2018-11-09 23:37:11\n",
      "Thread: 6\n",
      "\n",
      "Grid Search will test 18.651428571428568 combinations.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-09 23:44:22\n",
      "Accuracy:  0 \n",
      "Params:  [0, 0, 0]\n",
      "Precision:  0 \n",
      "Params:  [0, 0, 0] \n",
      "\n",
      "----------------------\n",
      "Thread: 1\n",
      "Params: 100,20,40\n",
      "Accuracy: 0.3983050847457627; Precision: 0.8010356992348519\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-09 23:44:44\n",
      "Accuracy:  0 \n",
      "Params:  [0, 0, 0]\n",
      "Precision:  0 \n",
      "Params:  [0, 0, 0] \n",
      "\n",
      "----------------------\n",
      "Thread: 4\n",
      "Params: 100,35,40\n",
      "Accuracy: 0.4576271186440678; Precision: 0.8559760429463821\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-09 23:47:59\n",
      "Accuracy:  0 \n",
      "Params:  [0, 0, 0]\n",
      "Precision:  0 \n",
      "Params:  [0, 0, 0] \n",
      "\n",
      "----------------------\n",
      "Thread: 2\n",
      "Params: 150,20,40\n",
      "Accuracy: 0.3898305084745763; Precision: 0.8027530340030341\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-09 23:48:35\n",
      "Accuracy:  0 \n",
      "Params:  [0, 0, 0]\n",
      "Precision:  0 \n",
      "Params:  [0, 0, 0] \n",
      "\n",
      "----------------------\n",
      "Thread: 5\n",
      "Params: 150,35,40\n",
      "Accuracy: 0.4562146892655367; Precision: 0.8535384812927186\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-09 23:51:30\n",
      "Accuracy:  0 \n",
      "Params:  [0, 0, 0]\n",
      "Precision:  0 \n",
      "Params:  [0, 0, 0] \n",
      "\n",
      "----------------------\n",
      "Thread: 3\n",
      "Params: 200,20,40\n",
      "Accuracy: 0.3997175141242938; Precision: 0.8103639494529324\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-09 23:51:31\n",
      "Accuracy:  0.3983050847457627 \n",
      "Params:  [100, 20, 40]\n",
      "Precision:  0.8010356992348519 \n",
      "Params:  [100, 20, 40] \n",
      "\n",
      "----------------------\n",
      "Thread: 1\n",
      "Params: 100,20,75\n",
      "Accuracy: 0.3983050847457627; Precision: 0.8010356992348519\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-09 23:52:19\n",
      "Accuracy:  0 \n",
      "Params:  [0, 0, 0]\n",
      "Precision:  0 \n",
      "Params:  [0, 0, 0] \n",
      "\n",
      "----------------------\n",
      "Thread: 6\n",
      "Params: 200,35,40\n",
      "Accuracy: 0.4632768361581921; Precision: 0.8552429248403826\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-09 23:52:20\n",
      "Accuracy:  0.4576271186440678 \n",
      "Params:  [100, 35, 40]\n",
      "Precision:  0.8559760429463821 \n",
      "Params:  [100, 35, 40] \n",
      "\n",
      "----------------------\n",
      "Thread: 4\n",
      "Params: 100,35,75\n",
      "Accuracy: 0.4576271186440678; Precision: 0.8559760429463821\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-09 23:58:20\n",
      "Accuracy:  0.3898305084745763 \n",
      "Params:  [150, 20, 40]\n",
      "Precision:  0.8027530340030341 \n",
      "Params:  [150, 20, 40] \n",
      "\n",
      "----------------------\n",
      "Thread: 2\n",
      "Params: 150,20,75\n",
      "Accuracy: 0.3898305084745763; Precision: 0.8027530340030341\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-09 23:58:31\n",
      "Accuracy:  0.3983050847457627 \n",
      "Params:  [100, 20, 75]\n",
      "Precision:  0.8010356992348519 \n",
      "Params:  [100, 20, 75] \n",
      "\n",
      "----------------------\n",
      "Thread: 1\n",
      "Params: 100,23,40\n",
      "Accuracy: 0.423728813559322; Precision: 0.8283271264203468\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-09 23:59:34\n",
      "Accuracy:  0.4562146892655367 \n",
      "Params:  [150, 35, 40]\n",
      "Precision:  0.8535384812927186 \n",
      "Params:  [150, 35, 40] \n",
      "\n",
      "----------------------\n",
      "Thread: 5\n",
      "Params: 150,35,75\n",
      "Accuracy: 0.4562146892655367; Precision: 0.8535384812927186\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-09 23:59:41\n",
      "Accuracy:  0.4576271186440678 \n",
      "Params:  [100, 35, 75]\n",
      "Precision:  0.8559760429463821 \n",
      "Params:  [100, 35, 75] \n",
      "\n",
      "----------------------\n",
      "Thread: 4\n",
      "Params: 100,38,40\n",
      "Accuracy: 0.4505649717514124; Precision: 0.8592083254159526\n",
      "\n"
     ]
    }
   ],
   "source": [
    "thread_list = []\n",
    "\n",
    "t1 = threading.Thread(target=gridResultRFC, args=(test_train_set, 100, 151, 25, 20, 36, 3, 1, 52, 25, 'log_rf_grid_1', 1))\n",
    "t2 = threading.Thread(target=gridResultRFC, args=(test_train_set, 150, 201, 25, 20, 36, 3, 1, 52, 25, 'log_rf_grid_2', 2))\n",
    "t3 = threading.Thread(target=gridResultRFC, args=(test_train_set, 200, 251, 25, 20, 36, 3, 1, 52, 25, 'log_rf_grid_3', 3))\n",
    "t4 = threading.Thread(target=gridResultRFC, args=(test_train_set, 100, 151, 25, 35, 51, 3, 1, 52, 25, 'log_rf_grid_4', 4))\n",
    "t5 = threading.Thread(target=gridResultRFC, args=(test_train_set, 150, 201, 25, 35, 51, 3, 1, 52, 25, 'log_rf_grid_5', 5))\n",
    "t6 = threading.Thread(target=gridResultRFC, args=(test_train_set, 200, 251, 25, 35, 51, 3, 1, 52, 25, 'log_rf_grid_6', 6))\n",
    "\n",
    "\n",
    "# Sticks the thread in a list so that it remains accessible\n",
    "thread_list.append(t1)\n",
    "thread_list.append(t2)\n",
    "thread_list.append(t3)\n",
    "thread_list.append(t4)\n",
    "thread_list.append(t5)\n",
    "thread_list.append(t6)\n",
    "\n",
    "# Starts threads\n",
    "for thread in thread_list:\n",
    "    thread.start()\n",
    "\n",
    "# This blocks the calling thread until the thread whose join() method is called is terminated.\n",
    "# From http://docs.python.org/2/library/threading.html#thread-objects\n",
    "for thread in thread_list:\n",
    "    thread.join()\n",
    "\n",
    "# Demonstrates that the main process waited for threads to complete\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rf = RandomForestClassifier(n_estimators=100, max_depth=15,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rf.fit(train_X, train_y)\n",
    "# pred_rf = rf.predict(test_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# kNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=1)\n",
    "knn.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_knn = knn.predict(test_X)\n",
    "print('Precision: ', metrics.precision_score(test_y, pred_knn, average=\"samples\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy: ', metrics.accuracy_score(test_y, pred_knn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OnevsRest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from numpy import array\n",
    "\n",
    "clf = OneVsRestClassifier(SVC(probability=True, gamma='auto'))\n",
    "clf.fit(train_X, train_y)\n",
    "predictions = clf.predict(test_X)\n",
    "\n",
    "my_metrics = metrics.classification_report(test_y, predictions)\n",
    "\n",
    "# print(my_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(clf.score(test_X, test_y, sample_weight=None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mclf = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(15,), random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mclf.fit(train_X, train_y)\n",
    "predictionsm = mclf.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Precision: ', metrics.precision_score(test_y, predictionsm,average='samples'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy: ', metrics.accuracy_score(test_y, predictionsm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
