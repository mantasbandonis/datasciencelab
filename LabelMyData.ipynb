{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/user1-2/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/user1-2/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/user1-2/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "2019-01-13 16:56:35\n",
      "_______________________________________________\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import csv\n",
    "import datetime\n",
    "import nltk\n",
    "import requests\n",
    "import _pickle\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from nltk import ngrams, FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "print(datetime.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d %H:%M:%S'))\n",
    "print('_______________________________________________')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip freeze > requirements_wu.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Def Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTxt(path):  \n",
    "    return open(path, 'r').read()\n",
    "\n",
    "# function to open csv files with the right encoding\n",
    "def getCsv(path, delim = ',', enc = 'utf-8'):          \n",
    "    list_return = []\n",
    "    with open (path, encoding = enc) as file:\n",
    "        csvreader = csv.reader(file, delimiter = delim)\n",
    "        for line in csvreader:\n",
    "            list_return.append(line)\n",
    "    return list_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_fillerwords = getTxt('..//input//fillerwords.txt').split(',') + stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizeString(title, desc):\n",
    "    print('Started at ' , datetime.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d %H:%M:%S.%f'))\n",
    "    print('Tokenisation is running.')\n",
    "    global list_fillerwords\n",
    "#     string_to_clean = title + ' ' + desc\n",
    "    string_to_clean = desc\n",
    "    porterstemmer = PorterStemmer()\n",
    "    lemmatizer = nltk.WordNetLemmatizer()\n",
    "    \n",
    "    list_token = nltk.word_tokenize(''.join([x for x in string_to_clean if not x.isdigit()]).replace('-',' ').replace('.','').replace(',','').replace('%','').replace(';',' ').replace('/', ' ').replace('(','').replace(')',''))\n",
    "    \n",
    "    for word in list_fillerwords:\n",
    "        while (word in list_token):\n",
    "            list_token.remove(word)\n",
    "            \n",
    "    for i, word in enumerate(list_token):\n",
    "        list_token[i] = list_token[i].lower()\n",
    "        lemmatizer.lemmatize(porterstemmer.stem(list_token[i]))\n",
    "    \n",
    "    print('Done at ' , datetime.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d %H:%M:%S.%f'))\n",
    "    return list_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for summing up single vectors (=words) in a vectorlist\n",
    "def sumVect(list_vect):\n",
    "    try:\n",
    "        for i, elem in enumerate(list_vect):\n",
    "            if not elem:\n",
    "                del list_vect[i]\n",
    "        \n",
    "        int_len_vect = len(list_vect[0])\n",
    "        list_vect_sum = [0] * int_len_vect\n",
    "        for vect in list_vect:\n",
    "            for i, dim in enumerate(vect):\n",
    "                list_vect_sum[i] += float(dim)\n",
    "        return(list_vect_sum)\n",
    "    except Exception as e:\n",
    "        print(list_vect)\n",
    "        print(i)\n",
    "        print(list_vect_sum)\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorizeStringList(list_string):\n",
    "    print('Vectorisation is running.')\n",
    "    adress = 'http://word2vec.ai.wu.ac.at/googlenews/model?word='\n",
    "    feat_words = []\n",
    "    \n",
    "    for str_elem in list_string:\n",
    "        word = str_elem.strip() \n",
    "        try:\n",
    "            feat_words.append([float(x) for x in requests.get((adress+word)).text.replace(' ','').replace('[','').replace(']','').split(',')])\n",
    "        except:\n",
    "            try:\n",
    "                feat_words.append([float(x) for x in requests.get((adress+word.title())).text.replace(' ','').replace('[','').replace(']','').split(',')])\n",
    "            except Exception as e:\n",
    "                feat_words.append([])\n",
    "                           \n",
    "    if feat_words:\n",
    "        list_return = sumVect(feat_words)\n",
    "        \n",
    "    print('Done at ' , datetime.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d %H:%M:%S.%f'))\n",
    "    return list_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labelDataRFC(vect):\n",
    "    print('Creating labels.')\n",
    "        \n",
    "    with open('dumped_randomforestclassifier.pkl', 'rb') as fid:\n",
    "        rf_load = _pickle.load(fid)\n",
    "    with open('dumped_binarizer.pkl', 'rb') as fid:\n",
    "        bin_load = _pickle.load(fid)\n",
    "    \n",
    "    np_in = [vect]\n",
    "    X = np.array(np_in)\n",
    "    pred_rf = rf_load.predict(X)\n",
    "    pred_label = bin_load.inverse_transform(pred_rf)\n",
    "    \n",
    "    print('Done at ' , datetime.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d %H:%M:%S.%f'))\n",
    "    return pred_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labelDataOVRkNN(vect):\n",
    "    print('Creating labels.')\n",
    "        \n",
    "    with open('dumped_ovrknn20.pkl', 'rb') as fid:\n",
    "        rf_load = _pickle.load(fid)\n",
    "    with open('dumped_binarizer.pkl', 'rb') as fid:\n",
    "        bin_load = _pickle.load(fid)\n",
    "    \n",
    "    np_in = [vect]\n",
    "    X = np.array(np_in)\n",
    "    pred_rf = rf_load.predict(X)\n",
    "    pred_label = bin_load.inverse_transform(pred_rf)\n",
    "    \n",
    "    print('Done at ' , datetime.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d %H:%M:%S.%f'))\n",
    "    return pred_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labelDataOVRRFC(vect):\n",
    "    print('Creating labels.')\n",
    "        \n",
    "    with open('dumped_ovrrfc.pkl', 'rb') as fid:\n",
    "        rf_load = _pickle.load(fid)\n",
    "    with open('dumped_binarizer.pkl', 'rb') as fid:\n",
    "        bin_load = _pickle.load(fid)\n",
    "    \n",
    "    np_in = [vect]\n",
    "    X = np.array(np_in)\n",
    "    pred_rf = rf_load.predict(X)\n",
    "    pred_label = bin_load.inverse_transform(pred_rf)\n",
    "    \n",
    "    print('Done at ' , datetime.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d %H:%M:%S.%f'))\n",
    "    return pred_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labelDataVoting(vect):\n",
    "    print('Creating labels.')\n",
    "        \n",
    "    with open('dumped_randomforestclassifier.pkl', 'rb') as fid:\n",
    "        rf_load = _pickle.load(fid)\n",
    "    with open('dumped_ovrrfc.pkl', 'rb') as fid:\n",
    "        ovrrf_load = _pickle.load(fid)\n",
    "    with open('dumped_ovrknn20.pkl', 'rb') as fid:\n",
    "        ovrknn_load = _pickle.load(fid)\n",
    "    with open('dumped_binarizer.pkl', 'rb') as fid:\n",
    "        bin_load = _pickle.load(fid)\n",
    "    \n",
    "    np_in = [vect]\n",
    "    X = np.array(np_in)\n",
    "    pred_rf = rf_load.predict(X)\n",
    "    pred_ovrrf = ovrrf_load.predict(X)\n",
    "    pred_ovrknn = ovrknn_load.predict(X)\n",
    "    \n",
    "    pred_label = ((pred_rf+pred_ovrrf+pred_ovrknn)/3)\n",
    "    for i, label in enumerate(pred_label[0]):\n",
    "        if label > 0.5:\n",
    "            pred_label[0][i] = 1\n",
    "        else:\n",
    "            pred_label[0][i] = 0\n",
    "        \n",
    "        \n",
    "    pred_out = bin_load.inverse_transform(pred_label)\n",
    "    \n",
    "    print('Done at ' , datetime.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d %H:%M:%S.%f'))\n",
    "    return pred_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = 'Labor force participation rate, female (% of female population ages 15+) (modeled ILO estimate), countries grouped by income levels'\n",
    "describtion = 'Labor force participation rate is the proportion of the population ages 15 and older that is economically active: all people who supply labor for the production of goods and services during a specified period.'\n",
    "# title = 'CO2 emissions (metric tons per capita)'\n",
    "# describtion = 'Carbon dioxide emissions are those stemming from the burning of fossil fuels and the manufacture of cement. They include carbon dioxide produced during consumption of solid, liquid, and gas fuels and gas flaring. Government reaction taxes issue '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started at  2019-01-13 16:56:35.329619\n",
      "Tokenisation is running.\n",
      "Done at  2019-01-13 16:56:39.053012\n",
      "Vectorisation is running.\n",
      "Done at  2019-01-13 16:56:39.179940\n",
      "Creating labels.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.6/site-packages/sklearn/base.py:311: UserWarning: Trying to unpickle estimator DecisionTreeClassifier from version 0.19.2 when using version 0.19.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "/opt/anaconda3/lib/python3.6/site-packages/sklearn/base.py:311: UserWarning: Trying to unpickle estimator RandomForestClassifier from version 0.19.2 when using version 0.19.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "/opt/anaconda3/lib/python3.6/site-packages/sklearn/base.py:311: UserWarning: Trying to unpickle estimator MultiLabelBinarizer from version 0.19.2 when using version 0.19.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done at  2019-01-13 16:56:46.541823\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[()]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labelDataRFC(vectorizeStringList(tokenizeString(title, describtion)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started at  2019-01-13 16:56:46.574544\n",
      "Tokenisation is running.\n",
      "Done at  2019-01-13 16:56:46.576316\n",
      "Vectorisation is running.\n",
      "Done at  2019-01-13 16:56:46.689278\n",
      "Creating labels.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.6/site-packages/sklearn/base.py:311: UserWarning: Trying to unpickle estimator DecisionTreeClassifier from version 0.19.2 when using version 0.19.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "/opt/anaconda3/lib/python3.6/site-packages/sklearn/base.py:311: UserWarning: Trying to unpickle estimator RandomForestClassifier from version 0.19.2 when using version 0.19.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "/opt/anaconda3/lib/python3.6/site-packages/sklearn/base.py:311: UserWarning: Trying to unpickle estimator LabelBinarizer from version 0.19.2 when using version 0.19.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "/opt/anaconda3/lib/python3.6/site-packages/sklearn/base.py:311: UserWarning: Trying to unpickle estimator _ConstantPredictor from version 0.19.2 when using version 0.19.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "/opt/anaconda3/lib/python3.6/site-packages/sklearn/base.py:311: UserWarning: Trying to unpickle estimator OneVsRestClassifier from version 0.19.2 when using version 0.19.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "/opt/anaconda3/lib/python3.6/site-packages/sklearn/base.py:311: UserWarning: Trying to unpickle estimator MultiLabelBinarizer from version 0.19.2 when using version 0.19.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done at  2019-01-13 16:57:26.494032\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(' ageing',\n",
       "  ' cpia',\n",
       "  ' demographic economics',\n",
       "  ' demography',\n",
       "  ' disaster_accident',\n",
       "  ' economics',\n",
       "  ' energy development',\n",
       "  ' energy economics',\n",
       "  ' environment',\n",
       "  ' expense',\n",
       "  ' female labor force in the muslim world',\n",
       "  ' government',\n",
       "  ' health',\n",
       "  ' health_medical_pharma',\n",
       "  ' hiv',\n",
       "  ' hospitality_recreation',\n",
       "  ' human interest',\n",
       "  ' human migration',\n",
       "  ' law_crime',\n",
       "  ' medicine',\n",
       "  ' money',\n",
       "  ' nature',\n",
       "  ' population',\n",
       "  ' religion_belief',\n",
       "  ' taxation in the united states',\n",
       "  ' technology_internet',\n",
       "  ' tertiary education',\n",
       "  ' unemployment',\n",
       "  ' united nations development group',\n",
       "  ' workforce',\n",
       "  ' world bank cpia',\n",
       "  ' world bank labor',\n",
       "  ' world bank number',\n",
       "  ' world bank population',\n",
       "  'disaster_accident',\n",
       "  'environment',\n",
       "  'health_medical_pharma',\n",
       "  'law_crime',\n",
       "  'macroeconomics',\n",
       "  'social issues',\n",
       "  'technology_internet')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labelDataOVRRFC(vectorizeStringList(tokenizeString(title, describtion)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started at  2019-01-13 16:57:26.632824\n",
      "Tokenisation is running.\n",
      "Done at  2019-01-13 16:57:26.636034\n",
      "Vectorisation is running.\n",
      "Done at  2019-01-13 16:57:26.762438\n",
      "Creating labels.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.6/site-packages/sklearn/base.py:311: UserWarning: Trying to unpickle estimator DecisionTreeClassifier from version 0.19.2 when using version 0.19.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "/opt/anaconda3/lib/python3.6/site-packages/sklearn/base.py:311: UserWarning: Trying to unpickle estimator RandomForestClassifier from version 0.19.2 when using version 0.19.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "/opt/anaconda3/lib/python3.6/site-packages/sklearn/base.py:311: UserWarning: Trying to unpickle estimator LabelBinarizer from version 0.19.2 when using version 0.19.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "/opt/anaconda3/lib/python3.6/site-packages/sklearn/base.py:311: UserWarning: Trying to unpickle estimator _ConstantPredictor from version 0.19.2 when using version 0.19.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "/opt/anaconda3/lib/python3.6/site-packages/sklearn/base.py:311: UserWarning: Trying to unpickle estimator OneVsRestClassifier from version 0.19.2 when using version 0.19.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "/opt/anaconda3/lib/python3.6/site-packages/sklearn/base.py:311: UserWarning: Trying to unpickle estimator KNeighborsClassifier from version 0.19.2 when using version 0.19.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "/opt/anaconda3/lib/python3.6/site-packages/sklearn/base.py:311: UserWarning: Trying to unpickle estimator MultiLabelBinarizer from version 0.19.2 when using version 0.19.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done at  2019-01-13 16:58:10.281054\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(' female labor force in the muslim world',\n",
       "  ' unemployment',\n",
       "  ' united nations development group',\n",
       "  ' workforce',\n",
       "  ' world bank labor',\n",
       "  'social issues')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labelDataVoting(vectorizeStringList(tokenizeString(title, describtion)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started at  2019-01-13 16:58:10.407445\n",
      "Tokenisation is running.\n",
      "Done at  2019-01-13 16:58:10.409517\n",
      "Vectorisation is running.\n",
      "Done at  2019-01-13 16:58:10.503655\n",
      "Creating labels.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.6/site-packages/sklearn/base.py:311: UserWarning: Trying to unpickle estimator KNeighborsClassifier from version 0.19.2 when using version 0.19.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "/opt/anaconda3/lib/python3.6/site-packages/sklearn/base.py:311: UserWarning: Trying to unpickle estimator LabelBinarizer from version 0.19.2 when using version 0.19.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "/opt/anaconda3/lib/python3.6/site-packages/sklearn/base.py:311: UserWarning: Trying to unpickle estimator _ConstantPredictor from version 0.19.2 when using version 0.19.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "/opt/anaconda3/lib/python3.6/site-packages/sklearn/base.py:311: UserWarning: Trying to unpickle estimator OneVsRestClassifier from version 0.19.2 when using version 0.19.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "/opt/anaconda3/lib/python3.6/site-packages/sklearn/base.py:311: UserWarning: Trying to unpickle estimator MultiLabelBinarizer from version 0.19.2 when using version 0.19.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done at  2019-01-13 16:58:11.475550\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(' female labor force in the muslim world',\n",
       "  ' international labour organization',\n",
       "  ' unemployment',\n",
       "  ' united nations',\n",
       "  ' united nations development group',\n",
       "  ' workforce',\n",
       "  ' world bank labor',\n",
       "  'social issues')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labelDataOVRkNN(vectorizeStringList(tokenizeString(title, describtion)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ...some more test cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "testcases = getCsv('single_cases2.csv', delim = ';', enc = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Case index  0\n",
      "Started at  2019-01-13 16:58:11.507201\n",
      "Tokenisation is running.\n",
      "Done at  2019-01-13 16:58:11.507937\n",
      "Vectorisation is running.\n",
      "Done at  2019-01-13 16:58:11.533153\n",
      "Creating labels.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.6/site-packages/sklearn/base.py:311: UserWarning: Trying to unpickle estimator DecisionTreeClassifier from version 0.19.2 when using version 0.19.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "/opt/anaconda3/lib/python3.6/site-packages/sklearn/base.py:311: UserWarning: Trying to unpickle estimator RandomForestClassifier from version 0.19.2 when using version 0.19.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "/opt/anaconda3/lib/python3.6/site-packages/sklearn/base.py:311: UserWarning: Trying to unpickle estimator MultiLabelBinarizer from version 0.19.2 when using version 0.19.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done at  2019-01-13 16:58:13.263672\n",
      "Started at  2019-01-13 16:58:13.280664\n",
      "Tokenisation is running.\n",
      "Done at  2019-01-13 16:58:13.282431\n",
      "Vectorisation is running.\n",
      "Done at  2019-01-13 16:58:13.307516\n",
      "Creating labels.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.6/site-packages/sklearn/base.py:311: UserWarning: Trying to unpickle estimator LabelBinarizer from version 0.19.2 when using version 0.19.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-aba5cc5be903>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0minner_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0minner_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabelDataRFC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectorizeStringList\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizeString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcase\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcase\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0minner_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabelDataOVRRFC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectorizeStringList\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizeString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcase\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcase\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0minner_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabelDataOVRkNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectorizeStringList\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizeString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcase\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcase\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0minner_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabelDataVoting\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectorizeStringList\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizeString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcase\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcase\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-86db0032b6e8>\u001b[0m in \u001b[0;36mlabelDataOVRRFC\u001b[0;34m(vect)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dumped_ovrrfc.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfid\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mrf_load\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_pickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dumped_binarizer.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfid\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mbin_load\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_pickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36m__setstate__\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m    300\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m__setstate__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__module__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'sklearn.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m             \u001b[0mpickle_version\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"_sklearn_version\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"pre-0.18\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "labels = []\n",
    "for i, case in enumerate(testcases):\n",
    "    print('Case index ', i)\n",
    "    inner_list = []\n",
    "    inner_list.append(labelDataRFC(vectorizeStringList(tokenizeString(case[1], case[0]))))\n",
    "    inner_list.append(labelDataOVRRFC(vectorizeStringList(tokenizeString(case[1], case[0]))))\n",
    "    inner_list.append(labelDataOVRkNN(vectorizeStringList(tokenizeString(case[1], case[0]))))\n",
    "    inner_list.append(labelDataVoting(vectorizeStringList(tokenizeString(case[1], case[0]))))\n",
    "    print(inner_list)\n",
    "    labels.append(inner_list)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('RandomForestClassifier ----- OvR RFC ----- OvR kNN ----- Voting Classifier\\n==================================================================')\n",
    "for i, line in enumerate(labels):\n",
    "    print('Case ', i, ':')\n",
    "    for elem in line:\n",
    "        print(elem, end=' - - ')\n",
    "    print('\\n==================================================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
