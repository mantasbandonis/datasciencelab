{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import platform\n",
    "import sys\n",
    "import requests\n",
    "import json\n",
    "import operator\n",
    "import time\n",
    "import datetime\n",
    "import nltk\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as pyplot\n",
    "\n",
    "if platform.system() == 'Windows':\n",
    "    if not os.path.exists('..\\\\output'):\n",
    "        os.makedirs('..\\\\output')   \n",
    "else:\n",
    "    if not os.path.exists('../output/'):\n",
    "        os.makedirs('../output/')\n",
    "    \n",
    "print(datetime.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d %H:%M:%S'))\n",
    "print('_______________________________________________')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ngrams, FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeText(text, path, mode = 'a'):\n",
    "    with open (path, mode) as textout:\n",
    "        textout.write((text))\n",
    "        \n",
    "def writeJson(json, path, mode = 'w'):\n",
    "    with open(path, mode) as file:\n",
    "        file.write(json.dumps(json))\n",
    "        \n",
    "def writeCsv(listOut, outputFile):\n",
    "    import csv\n",
    "    with open (outputFile, \"w\", newline='') as outputfile:\n",
    "        writer = csv.writer(outputfile, delimiter = \",\")\n",
    "        for element in listOut:\n",
    "            writer.writerow(element)\n",
    "\n",
    "def getTxt(path):\n",
    "    return open(path, 'r').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getToken():\n",
    "    if platform.system() == 'Windows':\n",
    "        return open('..\\\\token\\\\token.txt', 'r').read()\n",
    "    else:\n",
    "        return open('../token/token.txt', 'r').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def getJsonDocs(page = 1, itemsperpage = 200):\n",
    "    jsondata ='dummy'\n",
    "    list_return = []\n",
    "\n",
    "    while jsondata:\n",
    "        try:\n",
    "            r = requests.get(\"https://app.23degrees.io/services/pub/api/v1/opendata/getWorldBankDataSetsPaginated/\"+str(page)+\"/\"+str(itemsperpage)\n",
    "                         ,timeout=None\n",
    "                         ,headers={  'Content-Type': 'application/json'\n",
    "                                    , 'Authorization' : 'Bearer '+ getToken()\n",
    "                                   }\n",
    "                        )    \n",
    "            jsondata = r.json()\n",
    "            for elem in jsondata:\n",
    "                list_return.append(elem)\n",
    "        except requests.exceptions.HTTPError as err:\n",
    "            print(err)\n",
    "        page += 1\n",
    "    return list_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFeaturesLabels(list_in):\n",
    "    list_return = []\n",
    "    for elem in list_in:\n",
    "        list_inner = []\n",
    "        list_inner.append(elem[1])\n",
    "        list_inner.append(elem[2])\n",
    "        list_inner.append(elem[3])\n",
    "        list_return.append(list_inner)\n",
    "    return list_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_js_docs = getJsonDocs(1,200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_js_docs[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_list_js_tags = [] #Format: jsondoc(str)|name(str)|description(str)|tags(list)\n",
    "list_js_tags_inner = []\n",
    "list_js_notags = []\n",
    "list_unique_tags = []\n",
    "list_tags = []\n",
    "dict_tags = {}\n",
    "for i, jsondoc in enumerate(list_js_docs):\n",
    "    jstmp = json.dumps(jsondoc, sort_keys=True,indent=4, separators=(',', ': '))\n",
    "    if 'tags' in jstmp:\n",
    "        list_js_tags_inner = []\n",
    "        list_js_tags_inner.append(jstmp)\n",
    "        tmp_list_tags = []\n",
    "        for tag in jsondoc['typeSpecific']['context']['tags']:\n",
    "            tmp_list_tags.append(tag)\n",
    "            list_tags.append(tag)\n",
    "            if tag not in list_unique_tags:\n",
    "                list_unique_tags.append(tag)\n",
    "                dict_tags[tag] = 1\n",
    "            else:\n",
    "                dict_tags[tag] += 1\n",
    "        list_js_tags_inner.append(jsondoc['name'])\n",
    "        list_js_tags_inner.append(jsondoc['description'])\n",
    "        list_js_tags_inner.append(tmp_list_tags)\n",
    "        list_list_js_tags.append(list_js_tags_inner)\n",
    "                \n",
    "    else:\n",
    "        list_js_notags.append(jstmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_tags_sorted = list(sorted(dict_tags.items(), key=lambda x: x[1], reverse = True))\n",
    "ll_name_desc_tags = getFeaturesLabels(list_list_js_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Json-Files: ', len(list_js_docs))\n",
    "print('Json-Files with tags: ', len(list_list_js_tags))\n",
    "print('Json-Files without tags: ', len(list_js_notags))\n",
    "print('Unique tags: ', len(list_tags_sorted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fdist_total_tags = FreqDist(list_tags)\n",
    "fdist_total_tags.most_common(100)\n",
    "if platform.system() == 'Windows':\n",
    "    writeCsv(fdist_total_tags.most_common(500), '..\\\\output\\\\tags.csv')\n",
    "else:\n",
    "    writeCsv(fdist_total_tags.most_common(100), '../output/tags.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyplot.figure(figsize=(20, 8))\n",
    "pyplot.ylim(0, 4000)\n",
    "fdist_total_tags.plot(100, title = 'Most common Tags')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which tags should be ignored?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only tags appearing less than 1000 times?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pyplot.figure(figsize=(20, 8))\n",
    "pyplot.ylim(0, 1000)\n",
    "fdist_total_tags.plot(25, title = 'Most common Tags')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* excluding first 3 tags\n",
    "* assumption: take the first 200 most common tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def chooseTags(list_tags,first,topx):\n",
    "    list_chosen = []\n",
    "    list_notchosen = []\n",
    "    for i, elem in enumerate(list_tags):\n",
    "        if (i > first) & (i < (topx+first+1)):\n",
    "            list_chosen.append(elem[0].lower())   \n",
    "        else:\n",
    "            list_notchosen.append(elem[0].lower()) \n",
    "        \n",
    "    return list_chosen, list_notchosen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_chosen_tags = chooseTags(list_tags_sorted,2,80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing not used tags and fillerwords from name and describtion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_fillerwords = getTxt('../input/fillerwords.txt').split(',') + stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# string cleaning 1\n",
    "porterstemmer = PorterStemmer()\n",
    "lemmatizer = nltk.WordNetLemmatizer()\n",
    "for i, elem in enumerate(ll_name_desc_tags):\n",
    "    #prep name\n",
    "    ll_name_desc_tags[i][0] = nltk.word_tokenize(''.join([x for x in elem[0] if not x.isdigit()]).replace('-',' ').replace('.','').replace(',','').replace('%','').replace('(','').replace(')',''))\n",
    "    for word in list_fillerwords:\n",
    "        while (word in ll_name_desc_tags[i][0]):\n",
    "            ll_name_desc_tags[i][0].remove(word)\n",
    "    for j, word in enumerate(ll_name_desc_tags[i][0]):\n",
    "        ll_name_desc_tags[i][0][j] = ll_name_desc_tags[i][0][j].lower()\n",
    "        lemmatizer.lemmatize(porterstemmer.stem(ll_name_desc_tags[i][0][j]))\n",
    "    # prep desc\n",
    "    ll_name_desc_tags[i][1] = nltk.word_tokenize(''.join([x for x in elem[1] if not x.isdigit()]).replace('-',' ').replace('.','').replace(',','').replace('%','').replace('(','').replace(')',''))\n",
    "    for word in list_fillerwords:\n",
    "        while (word in ll_name_desc_tags[i][1]):\n",
    "            ll_name_desc_tags[i][1].remove(word)\n",
    "    for j, word in enumerate(ll_name_desc_tags[i][1]):\n",
    "        ll_name_desc_tags[i][1][j] = ll_name_desc_tags[i][1][j].lower()\n",
    "        lemmatizer.lemmatize(porterstemmer.stem(ll_name_desc_tags[i][1][j]))\n",
    "        \n",
    "    # lower labels\n",
    "    for j, word in enumerate(ll_name_desc_tags[i][2]):\n",
    "        ll_name_desc_tags[i][2][j] = ll_name_desc_tags[i][2][j].lower()   \n",
    "        \n",
    "    # removing not chosen labels\n",
    "    for tag in list_chosen_tags[1]:\n",
    "        while (tag in ll_name_desc_tags[i][2]):\n",
    "            ll_name_desc_tags[i][2].remove(tag)\n",
    "        \n",
    "    # split labels        \n",
    "    list_1_tmp_tags = []\n",
    "    for j, word in enumerate(ll_name_desc_tags[i][2]):\n",
    "        if '-' in word:\n",
    "            list_2_tmp_tags = word.split('-')\n",
    "        elif '_' in word:\n",
    "            list_2_tmp_tags = word.split('_')\n",
    "        else:\n",
    "            list_2_tmp_tags = word.split(' ')\n",
    "        for sword in list_2_tmp_tags:\n",
    "            list_1_tmp_tags.append(sword.lower())\n",
    "            \n",
    "    # finalise labels\n",
    "    ll_name_desc_tags[i][2] = []\n",
    "    for word in list_1_tmp_tags:\n",
    "        ll_name_desc_tags[i][2].append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deckungsqoute Berechnen\n",
    "cnt = 0\n",
    "cnt2 = 0\n",
    "cnt3 = 0\n",
    "print('Anzahl der Labels: ', len(list_chosen_tags[0]))\n",
    "for i, elem in enumerate(ll_name_desc_tags):\n",
    "    if not ll_name_desc_tags[i][2]:\n",
    "        cnt += 1\n",
    "    if len(ll_name_desc_tags[i][2]) < 3:\n",
    "        cnt2 += 1\n",
    "    if len(ll_name_desc_tags[i][2]) < 4:\n",
    "        cnt3 += 1\n",
    "        \n",
    "        \n",
    "print('Anzahl an ungedeckten Datensätzen: ', cnt)\n",
    "print('Anteil der gedeckten Datensätze: ', 1-(cnt/len(ll_name_desc_tags)))\n",
    "print('Anzahl der Datensätze mit weniger als 3 Label: ', cnt2)\n",
    "print('Anteil der Datensätze mit weniger als 3 Label: ', 1-(cnt2/len(ll_name_desc_tags)))\n",
    "print('Anzahl der Datensätze mit weniger als 4 Label: ', cnt3)\n",
    "print('Anteil der Datensätze mit weniger als 4 Label: ', 1-(cnt3/len(ll_name_desc_tags)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # string cleaning 2\n",
    "# for i, elem in enumerate(ll_name_desc_tags):\n",
    "#     #prep name\n",
    "#     ll_name_desc_tags[i][0] = nltk.word_tokenize(''.join([x for x in elem[0] if not x.isdigit()]).replace('.','').replace(',','').replace('%','').replace('(','').replace(')',''))\n",
    "#     for word in list_fillerwords:\n",
    "#         while (word in ll_name_desc_tags[i][0]):\n",
    "#             ll_name_desc_tags[i][0].remove(word)\n",
    "#     for j, word in enumerate(ll_name_desc_tags[i][0]):\n",
    "#         ll_name_desc_tags[i][0][j] = ll_name_desc_tags[i][0][j].lower()\n",
    "#         lemmatizer.lemmatize(porterstemmer.stem(ll_name_desc_tags[i][0][j]))\n",
    "#     # prep desc\n",
    "#     ll_name_desc_tags[i][1] = nltk.word_tokenize(''.join([x for x in elem[1] if not x.isdigit()]).replace('.','').replace(',','').replace('%','').replace('(','').replace(')',''))\n",
    "#     for word in list_fillerwords:\n",
    "#         while (word in ll_name_desc_tags[i][1]):\n",
    "#             ll_name_desc_tags[i][1].remove(word)\n",
    "#     for j, word in enumerate(ll_name_desc_tags[i][1]):\n",
    "#         ll_name_desc_tags[i][1][j] = ll_name_desc_tags[i][1][j].lower()\n",
    "#         lemmatizer.lemmatize(porterstemmer.stem(ll_name_desc_tags[i][1][j])) \n",
    "#     # lower labels\n",
    "#     for j, word in enumerate(ll_name_desc_tags[i][2]):\n",
    "#         ll_name_desc_tags[i][2][j] = ll_name_desc_tags[i][2][j].lower()    \n",
    "#     # removing not chosen labels\n",
    "#     for tag in list_chosen_tags[1]:\n",
    "#         while (tag in ll_name_desc_tags[i][2]):\n",
    "#             ll_name_desc_tags[i][2].remove(tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if platform.system() == 'Windows':\n",
    "    writeCsv(ll_name_desc_tags, '..\\\\output\\\\prep_out.csv')\n",
    "else:\n",
    "    writeCsv(ll_name_desc_tags, '../output/prep_out.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(datetime.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d %H:%M:%S'))\n",
    "print('\\n\\n_______________________________________________')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
